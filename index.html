<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <!--[if IE]>
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <![endif]-->
    <title>ERA Dataset</title>
    <!-- BOOTSTRAP CORE STYLE CSS -->
    <link href="assets/css/bootstrap.css" rel="stylesheet" />
    <!-- Font-Awesome Icons Styles -->
    <link href="assets/css/font-awesome.css" rel="stylesheet" />
     <!-- Pretty Photo For PopUp Styles -->
    <link href="assets/css/prettyPhoto.css" rel="stylesheet" />
    <!-- CUSTOM STYLE CSS -->
    <link href="assets/css/custom.css" rel="stylesheet" />    
    <!-- GOOGLE FONT -->
    <link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css' />
<style>
#myVideo {
  position: absolute;
  top: 0;
  left: 0;
  z-index: -100;
  width: 100%;
  height: 100%;
  object-fit: cover;
}
#myVideo2 {
  position: relative;
  top: 0;
  left: 0;
  z-index: 1;
  width: 100%;
  height: 100%;
}
</style>
</head>
<body  >
		 <section id="home-sec">
		 <video autoplay muted loop id="myVideo"><source src="assets/img/background.mp4" type="video/mp4"></video>
		   <div class="overlay"  >
                      <h1 >ERA DATASET </h1> 
                         <h4  >
										A Dataset and Deep Learning Benchmark for Event Recognition in Aerial Videos
                        </h4>
           </div>
		   
		 </section>

    <!-- HOME/VEDIO SECTION END-->
       <section id="Introduction-sec"  >

           
            <div class="container basic-set" >
                <div class="row text-center">
                    <div class="col-md-12">
                        <h3 class="head-line"><span ></span>   Introduction <span > </span> </h3>
                        
                    </div>
                </div>
                <div class="txt-block">

									<p>
										Along with the now widespread availability of unmanned aerial vehicles (UAVs), large volumes of aerial videos have been produced. It is unrealistic for humans to screen such big data and understand their contents. Hence methodological research on the automatic understanding of UAV videos is of paramount importance. In this work, we introduce a novel task of event recognition in unconstrained aerial videos in the remote sensing community and present a large-scale, human-annotated dataset, named ERA (Event Recognition in Aerial videos), consisting of 2,864 videos each with a label from 25 different classes corresponding to an event unfolding 5 seconds. The ERA dataset is designed to have a significant intra-class variation and inter-class similarity and captures dynamic events in different environments and at different scales. Moreover, in order to provide a benchmark for this task, we extensively evaluate existing deep networks. We expect that the ERA dataset will facilitate further progress in automatic aerial video comprehension.
									</p>
                      </div>
           </div>
           </section>
		   
		   <section id="Statistics-sec"  >

           
            <div class="container basic-set" >
                <div class="row text-center">
                    <div class="col-md-12">
                        <h3 class="head-line"><span ></span>   Statistics <span > </span> </h3>
                        
                    </div>
                </div>
                <div class="txt-block">

									<p>
											The  goal  of  this  work  is  to  collect  a  large,  diverse  datasetfor  training  models  for  event  understanding  in  UAV  videos. Since we gather aerial videos from Youtube, the largest videosharing  platform  in  the  world,  we  are  capable  of  includinga  large  breadth  of  diversity  that  would  be  more  challengingthan making use of self-collected dat. In total, wehave gathered and annotated 2,864 videos for 25 classes. Eachvideo sequence is at 24 fps (frames per second), in 5 seconds,and with a spatial size of 640×640 pixels.
									</p>
                      </div>
					  
					  <div class="portfolio-item HTML mix_all" data-cat="HTML" style="  display: inline-block; opacity: 1;">


                        <img src="assets/img/category.png" class="img-responsive " alt="" />
                    </div>
           </div>
           </section>
		   
		   <section id="Demo-sec"  >

           
            <div class="container basic-set" >
                <div class="row text-center">
                    <div class="col-md-12">
                        <h3 class="head-line"><span ></span>   Demo <span > </span> </h3>
                        
                    </div>
                </div>
				<video   id="myVideo2" controls ><source src="assets/img/demo_final_3.mp4" type="video/mp4" ></video>
				</div>
           </section>
		   <section id="Results-sec"  >

           
            <div class="container basic-set" >
                <div class="row text-center">
                    <div class="col-md-12">
                        <h3 class="head-line"><span ></span>   Results <span > </span> </h3>               
                    </div>
               </div>
	       <div class="portfolio-item HTML mix_all" data-cat="HTML" style="  display: inline-block; opacity: 1;">
                   <span class="caption">1) Confusion  matrix  of  TRN  for  the  ERA  dataset.</span>
                   <img src="assets/img/cf_trn.png" class="img-responsive " alt="" />
               </div>
	       <div class="portfolio-item HTML mix_all" data-cat="HTML" style="  display: inline-block; opacity: 1;">
                   <span class="caption">2) Confusion  matrix  of  Dense201  for  the  ERA  dataset.</span>
                   <img src="assets/img/cf_dense201.png" class="img-responsive " alt="" />
	       </div>
	       <div class="portfolio-item HTML mix_all" data-cat="HTML" style="  display: inline-block; opacity: 1;">
		   <span class="caption">3) <strong>Examples of event recognition results on the ERA dataset.</strong> We show the best two single-frame classification network architectures (i.e., Inception-v3 and DenseNet-201) and the best two video classification network architectures (i.e., I3D and TRN). The ground truth label and top 3 predictions of each model are reported. For each example, we show the first (left) and last (right) frames. Best viewed zoomed in color.</span>
                   <img src="assets/img/results1.png" class="img-responsive " alt="" />
               </div>
	       <div class="portfolio-item HTML mix_all" data-cat="HTML" style="  display: inline-block; opacity: 1;">
		   <span class="caption">4) <strong>Examples of misclassifications.</strong> We show several failure examples where the prediction is not in the top 3.</span>
                   <img src="assets/img/results2.png" class="img-responsive " alt="" />
	       </div>				
	   </div>
					   
           </section>
		   <section id="Download-sec"  >

           
            <div class="container basic-set" >
                <div class="row text-center">
                    <div class="col-md-12">
                        <h3 class="head-line"><span ></span>   Download <span > </span> </h3>
                        
                    </div>
                </div>
				<div class="txt-block">

									<p>
											Coming Soon...
									</p>
                      </div>
				
				</div>
           </section>
		   

          
       <!-- CONTACT  SECTION END-->

    <!-- JAVASCRIPT FILES PLACED AT THE BOTTOM TO REDUCE THE LOADING TIME  -->
    <!-- BOOTSTRAP SCRIPTS  -->
    <script src="assets/js/bootstrap.js"></script>
     <!-- EASING SCROLL SCRIPTS PLUGIN  -->
    <script src="assets/js/custom.js"></script>
    
</body>
</html>
